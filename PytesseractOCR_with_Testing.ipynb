{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MODUL OCR - SISTEM VERIFIKASI DOKUMEN OTOMATIS\n\n**Versi Windows 11**\n\n---\n\n## Deskripsi\nEkstraksi teks dari dokumen PDF/Gambar menggunakan Tesseract OCR\n\n**Alur Kerja:**\n```\nInput Gambar ‚Üí Orientation/Skew Correction ‚Üí Grayscale ‚Üí Adaptive Denoise ‚Üí Tesseract OCR (Grayscale, PSM 3, OEM 3) ‚Üí Post-processing (garbage line removal) ‚Üí Single-line Evaluation\n```\n\n**Catatan Pipeline:**\n- CLAHE, Sharpening, Otsu Thresholding, dan Morphology **dinonaktifkan** (pass-through)\n- Tesseract LSTM (OEM 3) bekerja lebih akurat pada gambar **grayscale** dibanding binary\n- **Post-processing**: Garbage line removal untuk membersihkan artefak watermark/stempel\n- Evaluasi menggunakan mode **single-line** (layout/newline diabaikan)\n\n**Target Performa:** <10 detik per dokumen\n\n**Framework:** Python + Tesseract OCR + OpenCV\n\nOCR System dengan Pengujian Akurasi, WER, dan CER\n\nDitambahkan: Testing menggunakan jiwer (alternatif FastWER yang lebih mudah di Windows)"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytesseract in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (1.17.0)\n",
      "Requirement already satisfied: Pillow in e:\\softwares\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (0.27.3)\n",
      "Requirement already satisfied: jiwer in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (4.0.0)\n",
      "Requirement already satisfied: packaging>=21.3 in e:\\softwares\\anaconda3\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in e:\\softwares\\anaconda3\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Requirement already satisfied: Levenshtein==0.27.3 in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (from python-Levenshtein) (0.27.3)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\agusi\\appdata\\roaming\\python\\python313\\site-packages (from Levenshtein==0.27.3->python-Levenshtein) (3.14.3)\n",
      "Requirement already satisfied: click>=8.1.8 in e:\\softwares\\anaconda3\\lib\\site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: colorama in e:\\softwares\\anaconda3\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "\n",
      "‚úÖ Instalasi package selesai!\n",
      "‚ö†Ô∏è  Pastikan Tesseract OCR dan Poppler sudah terinstall!\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# INSTALASI PYTHON PACKAGES\n",
    "# =============================================\n",
    "\n",
    "!pip install pytesseract pdf2image Pillow opencv-python python-Levenshtein jiwer\n",
    "\n",
    "print(\"\\n‚úÖ Instalasi package selesai!\")\n",
    "print(\"‚ö†Ô∏è  Pastikan Tesseract OCR dan Poppler sudah terinstall!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tesseract ditemukan di: C:\\Program Files\\Tesseract-OCR\\tesseract.exe\n",
      "‚úÖ Poppler ditemukan di: C:\\Program Files\\poppler-25.07.0\\Library\\bin\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# KONFIGURASI PATH TESSERACT & POPPLER\n",
    "# ================================================\n",
    "import os\n",
    "\n",
    "# SESUAIKAN PATH INI DENGAN LOKASI INSTALASI ANDA!\n",
    "TESSERACT_PATH = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "POPPLER_PATH = r'C:\\Program Files\\poppler-25.07.0\\Library\\bin'\n",
    "\n",
    "# Verifikasi path Tesseract\n",
    "if os.path.exists(TESSERACT_PATH):\n",
    "    print(f\"‚úÖ Tesseract ditemukan di: {TESSERACT_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Tesseract TIDAK ditemukan di: {TESSERACT_PATH}\")\n",
    "    print(\"‚ö†Ô∏è  Sesuaikan TESSERACT_PATH dengan lokasi instalasi Anda!\")\n",
    "\n",
    "# Verifikasi path Poppler\n",
    "if os.path.exists(POPPLER_PATH):\n",
    "    print(f\"‚úÖ Poppler ditemukan di: {POPPLER_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Poppler TIDAK ditemukan di: {POPPLER_PATH}\")\n",
    "    print(\"‚ö†Ô∏è  Sesuaikan POPPLER_PATH dengan lokasi instalasi Anda!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semua library berhasil di-import\n",
      "üì¶ Menggunakan jiwer untuk WER dan CER calculation\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# IMPORT LIBRARY DENGAN ERROR HANDLING\n",
    "# ================================================\n",
    "try:\n",
    "    import pytesseract\n",
    "    from pdf2image import convert_from_path\n",
    "    from IPython.display import display\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    import os\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import Levenshtein\n",
    "    import re\n",
    "    import json\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    from jiwer import wer, cer  # Import jiwer untuk WER dan CER\n",
    "\n",
    "    # Set Tesseract path untuk Windows\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "    print(\"‚úÖ Semua library berhasil di-import\")\n",
    "    print(\"üì¶ Menggunakan jiwer untuk WER dan CER calculation\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing library: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Pastikan semua library sudah terinstall\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tesseract engine version: 5.5.0.20241111\n",
      "\n",
      "üìö Bahasa yang tersedia: eng, ind, osd\n",
      "‚úÖ Bahasa Indonesia tersedia\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# VERIFIKASI TESSERACT OCR ENGINE\n",
    "# ================================================\n",
    "try:\n",
    "    tesseract_version = pytesseract.get_tesseract_version()\n",
    "    print(f\"‚úÖ Tesseract engine version: {tesseract_version}\")\n",
    "    \n",
    "    # Cek bahasa yang tersedia\n",
    "    languages = pytesseract.get_languages()\n",
    "    print(f\"\\nüìö Bahasa yang tersedia: {', '.join(languages)}\")\n",
    "    \n",
    "    if 'ind' in languages:\n",
    "        print(\"‚úÖ Bahasa Indonesia tersedia\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Bahasa Indonesia tidak tersedia\")\n",
    "        print(\"   Download tessdata dari: https://github.com/tesseract-ocr/tessdata\")\n",
    "        \n",
    "except pytesseract.TesseractNotFoundError:\n",
    "    print(\"‚ùå Tesseract tidak ditemukan!\")\n",
    "    print(f\"   Path yang dicoba: {TESSERACT_PATH}\")\n",
    "    print(\"   Pastikan Tesseract sudah terinstall dan path sudah benar\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Membaca ground truth dari file lokal...\n",
      "============================================================\n",
      "‚úÖ Folder ditemukan: E:\\Softwares\\Jupyter\\Projects\\OCR\\data\\dokumen\\listrik\\ground_truth\n",
      "\n",
      "‚úÖ BANDARBARU (Listrik).txt (940 karakter)\n",
      "   ‚Üí Key: BANDARBARU (Listrik).pdf\n",
      "‚úÖ MEDANPAYAGELI (Listrik).txt (942 karakter)\n",
      "   ‚Üí Key: MEDANPAYAGELI (Listrik).pdf\n",
      "‚úÖ NAMORAMBE (Listrik).txt (945 karakter)\n",
      "   ‚Üí Key: NAMORAMBE (Listrik).pdf\n",
      "‚úÖ PALANGGA (Listrik).txt (638 karakter)\n",
      "   ‚Üí Key: PALANGGA (Listrik).pdf\n",
      "‚úÖ PANCURBATU (Listrik).txt (947 karakter)\n",
      "   ‚Üí Key: PANCURBATU (Listrik).pdf\n",
      "\n",
      "============================================================\n",
      "üìä Total ground truth dimuat: 5 file\n",
      "============================================================\n",
      "\n",
      "üìã Daftar ground truth yang tersedia:\n",
      "   ‚Ä¢ BANDARBARU (Listrik).pdf: 940 karakter, 32 baris\n",
      "   ‚Ä¢ MEDANPAYAGELI (Listrik).pdf: 942 karakter, 32 baris\n",
      "   ‚Ä¢ NAMORAMBE (Listrik).pdf: 945 karakter, 32 baris\n",
      "   ‚Ä¢ PALANGGA (Listrik).pdf: 638 karakter, 27 baris\n",
      "   ‚Ä¢ PANCURBATU (Listrik).pdf: 947 karakter, 33 baris\n",
      "\n",
      "‚ö†Ô∏è  PENTING:\n",
      "   1. Pastikan nama file .txt sesuai dengan nama dokumen yang akan di-OCR\n",
      "   2. Contoh: 'Struk 1.txt' untuk 'Struk 1.pdf'\n",
      "   3. Ground truth harus berisi teks RAW tanpa normalisasi\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# KONFIGURASI GROUND TRUTH DARI FILE TXT LOKAL\n",
    "# ================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# SESUAIKAN PATH INI KE FOLDER GROUND TRUTH ANDA\n",
    "# GROUND_TRUTH_FOLDER = r'E:\\Softwares\\Jupyter\\Projects\\OCR\\dokumen\\dokumen_normal\\ground_truth'\n",
    "BASE_FOLDER = r'E:\\Softwares\\Jupyter\\Projects\\OCR\\data\\dokumen'\n",
    "\n",
    "# User tinggal pilih folder kategori\n",
    "CATEGORY = 'listrik'  # atau 'dokumen_blur', 'dokumen_noisy', dll\n",
    "GROUND_TRUTH_FOLDER = os.path.join(BASE_FOLDER, CATEGORY, 'ground_truth')\n",
    "DOCUMENTS_FOLDER = os.path.join(BASE_FOLDER, CATEGORY)\n",
    "\n",
    "print(\"üîç Membaca ground truth dari file lokal...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "GROUND_TRUTH = {}\n",
    "\n",
    "# Cek apakah folder exists\n",
    "if not os.path.exists(GROUND_TRUTH_FOLDER):\n",
    "    print(f\"‚ùå Folder tidak ditemukan: {GROUND_TRUTH_FOLDER}\")\n",
    "    print(\"‚ö†Ô∏è  Sesuaikan GROUND_TRUTH_FOLDER dengan lokasi folder Anda\")\n",
    "else:\n",
    "    print(f\"‚úÖ Folder ditemukan: {GROUND_TRUTH_FOLDER}\\n\")\n",
    "    \n",
    "    # Baca semua file .txt di folder\n",
    "    txt_files = list(Path(GROUND_TRUTH_FOLDER).glob('*.txt'))\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(f\"‚ö†Ô∏è  Tidak ada file .txt ditemukan di folder\")\n",
    "    else:\n",
    "        for txt_file in txt_files:\n",
    "            try:\n",
    "                # Baca isi file\n",
    "                with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Dapatkan nama file tanpa path\n",
    "                filename_base = txt_file.stem  # Nama file tanpa extension\n",
    "                \n",
    "                # Cari file dokumen yang sesuai di folder yang sama\n",
    "                import glob\n",
    "                doc_folder = os.path.dirname(GROUND_TRUTH_FOLDER)\n",
    "                \n",
    "                # Coba cocokkan dengan PDF atau gambar\n",
    "                possible_extensions = ['.pdf', '.jpg', '.jpeg', '.png']\n",
    "                actual_file = None\n",
    "                \n",
    "                for ext in possible_extensions:\n",
    "                    pattern = os.path.join(doc_folder, f\"{filename_base}{ext}\")\n",
    "                    matches = glob.glob(pattern)\n",
    "                    if matches:\n",
    "                        actual_file = os.path.basename(matches[0])\n",
    "                        break\n",
    "                \n",
    "                # Jika tidak ketemu file asli, gunakan PDF sebagai default\n",
    "                if actual_file is None:\n",
    "                    actual_file = f\"{filename_base}.pdf\"\n",
    "                \n",
    "                key = actual_file\n",
    "                GROUND_TRUTH[key] = content\n",
    "                \n",
    "                char_count = len(content)\n",
    "                print(f\"‚úÖ {txt_file.name} ({char_count} karakter)\")\n",
    "                print(f\"   ‚Üí Key: {key}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error membaca {txt_file.name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üìä Total ground truth dimuat: {len(GROUND_TRUTH)} file\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if GROUND_TRUTH:\n",
    "    print(\"\\nüìã Daftar ground truth yang tersedia:\")\n",
    "    for filename, content in GROUND_TRUTH.items():\n",
    "        char_count = len(content)\n",
    "        line_count = content.count('\\n') + 1\n",
    "        print(f\"   ‚Ä¢ {filename}: {char_count} karakter, {line_count} baris\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  PENTING:\")\n",
    "    print(\"   1. Pastikan nama file .txt sesuai dengan nama dokumen yang akan di-OCR\")\n",
    "    print(\"   2. Contoh: 'Struk 1.txt' untuk 'Struk 1.pdf'\")\n",
    "    print(\"   3. Ground truth harus berisi teks RAW tanpa normalisasi\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Tidak ada ground truth yang dimuat!\")\n",
    "    print(\"   Silakan periksa:\")\n",
    "    print(f\"   1. Path folder: {GROUND_TRUTH_FOLDER}\")\n",
    "    print(\"   2. Pastikan ada file .txt di folder tersebut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ditemukan 5 dokumen\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# INPUT FILE\n",
    "# ================================================\n",
    "# ‚úÖ SOLUSI (Otomatis - scan folder)\n",
    "from pathlib import Path\n",
    "\n",
    "def get_all_documents(folder_path, extensions=['.pdf', '.jpg', '.jpeg', '.png']):\n",
    "    \"\"\"Otomatis ambil semua file dokumen dari folder\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    all_files = []\n",
    "    \n",
    "    for ext in extensions:\n",
    "        all_files.extend(folder.glob(f'*{ext}'))\n",
    "        # all_files.extend(folder.glob(f'*{ext.upper()}'))\n",
    "    \n",
    "    return sorted([str(f) for f in all_files])\n",
    "\n",
    "# Pakai:\n",
    "DOCUMENTS_FOLDER = r'E:\\Softwares\\Jupyter\\Projects\\OCR\\data\\dokumen\\listrik'\n",
    "FILE_PATHS = get_all_documents(DOCUMENTS_FOLDER)\n",
    "print(f\"‚úÖ Ditemukan {len(FILE_PATHS)} dokumen\")\n",
    "# Output: ‚úÖ Ditemukan 50 dokumen (otomatis!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# KONVERSI FILE KE GAMBAR\n# ================================================\n\nprint(\"üîÑ Memproses file...\")\nprint(\"=\" * 60)\n\n# Inisialisasi timer global & tracking per dokumen\nocr_pipeline_start = time.time()\ntiming_per_doc = {}\n\ndef _record_timing(doc_name, step, duration):\n    \"\"\"Catat waktu proses per dokumen per tahap\"\"\"\n    if doc_name not in timing_per_doc:\n        timing_per_doc[doc_name] = {}\n    timing_per_doc[doc_name][step] = timing_per_doc[doc_name].get(step, 0) + duration\n\nall_images = []\nfile_info = []  # Track source file untuk setiap gambar\n\nfor file_path in FILE_PATHS:\n    print(f\"\\nüìÑ Memproses: {file_path}\")\n    _t_doc = time.time()\n    \n    try:\n        if file_path.lower().endswith('.pdf'):\n            # Konversi PDF ke gambar\n            images = convert_from_path(\n                file_path,\n                dpi=300,\n                poppler_path=POPPLER_PATH\n            )\n            print(f\"   ‚úÖ PDF dikonversi ke {len(images)} halaman\")\n        else:\n            # Baca file gambar langsung\n            images = [Image.open(file_path)]\n            print(f\"   ‚úÖ Gambar berhasil dibaca\")\n        \n        # Simpan semua gambar dan info file\n        for img in images:\n            all_images.append(img)\n            file_info.append(os.path.basename(file_path))\n        \n        _record_timing(os.path.basename(file_path), 'Konversi', time.time() - _t_doc)\n        \n        # Preview halaman pertama\n        if images:\n            print(f\"\\n   üîç Preview (Halaman 1):\")\n            display(images[0])\n            \n    except Exception as e:\n        print(f\"   ‚ùå Error: {e}\")\n        continue\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"üìä Total gambar siap diproses: {len(all_images)}\")\nprint(f\"{'=' * 60}\")\n\nimages = all_images"
  },
  {
   "cell_type": "code",
   "source": "# ================================================\n# PREPROCESSING 0: ORIENTATION & SKEW CORRECTION\n# ================================================\nimport matplotlib.pyplot as plt\n\nMAX_DISPLAY_ORIENT = 5\n\nprint(\"üîÑ Memulai proses Orientation & Skew Correction...\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\ncorrected_images = []\n\nfor i, img in enumerate(images):\n    _t_doc = time.time()\n    source_file = file_info[i]\n    rotation_applied = False\n    skew_applied = False\n    rotation_angle = 0\n    rotation_conf = 0.0\n    skew_angle = 0.0\n    osd_error = None\n\n    # === STEP 1: Deteksi & Koreksi Rotasi (OSD) ===\n    try:\n        osd = pytesseract.image_to_osd(img)\n        for line in osd.split('\\n'):\n            if 'Rotate:' in line:\n                rotation_angle = int(line.split(':')[-1].strip())\n            if 'Orientation confidence:' in line:\n                rotation_conf = float(line.split(':')[-1].strip())\n    except pytesseract.TesseractError as e:\n        osd_error = str(e)\n\n    # Rotasi jika terdeteksi (confidence > 1.0)\n    if osd_error is None and rotation_angle != 0 and rotation_conf > 1.0:\n        img = img.rotate(rotation_angle, expand=True, fillcolor=(255, 255, 255))\n        rotation_applied = True\n\n    # === STEP 1b: Fallback Rotasi 180¬∞ ===\n    # Jika OSD gagal atau tidak mendeteksi rotasi, cek apakah dokumen terbalik\n    # dengan membandingkan OCR confidence normal vs rotasi 180¬∞\n    if not rotation_applied:\n        try:\n            img_np_temp = np.array(img)\n            h, w = img_np_temp.shape[:2]\n            # Ambil crop tengah untuk tes cepat\n            center_crop = img_np_temp[h//4:3*h//4, w//4:3*w//4]\n            \n            # Confidence orientasi normal\n            data_normal = pytesseract.image_to_data(\n                Image.fromarray(center_crop),\n                lang='ind+eng',\n                config='--psm 6 --oem 3',\n                output_type=pytesseract.Output.DICT\n            )\n            conf_normal = [int(c) for c in data_normal['conf'] if int(c) > 0]\n            avg_conf_normal = sum(conf_normal) / len(conf_normal) if conf_normal else 0\n            \n            # Confidence rotasi 180¬∞\n            rotated_180 = np.rot90(img_np_temp, 2)\n            center_crop_180 = rotated_180[h//4:3*h//4, w//4:3*w//4]\n            data_rotated = pytesseract.image_to_data(\n                Image.fromarray(center_crop_180),\n                lang='ind+eng',\n                config='--psm 6 --oem 3',\n                output_type=pytesseract.Output.DICT\n            )\n            conf_rotated = [int(c) for c in data_rotated['conf'] if int(c) > 0]\n            avg_conf_rotated = sum(conf_rotated) / len(conf_rotated) if conf_rotated else 0\n            \n            # Rotasi 180¬∞ jika confidence jauh lebih tinggi\n            if avg_conf_rotated > avg_conf_normal + 10:\n                img = Image.fromarray(rotated_180)\n                rotation_applied = True\n                rotation_angle = 180\n                rotation_conf = avg_conf_rotated\n        except:\n            pass  # Fallback gagal, lanjut dengan orientasi asli\n\n    # === STEP 2: Deteksi & Koreksi Skew (Kemiringan Kecil) ===\n    img_np = np.array(img)\n    gray_temp = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY) if len(img_np.shape) == 3 else img_np\n\n    _, binary = cv2.threshold(gray_temp, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    coords = np.column_stack(np.where(binary > 0))\n\n    if len(coords) >= 10:\n        angle = cv2.minAreaRect(coords)[-1]\n        skew_angle = -(90 + angle) if angle < -45 else -angle\n        if abs(skew_angle) > 15:\n            skew_angle = 0.0\n\n    if abs(skew_angle) >= 0.5:\n        (h, w) = img_np.shape[:2]\n        M = cv2.getRotationMatrix2D((w // 2, h // 2), skew_angle, 1.0)\n        border = (255, 255, 255) if len(img_np.shape) == 3 else 255\n        img_np = cv2.warpAffine(img_np, M, (w, h), flags=cv2.INTER_CUBIC,\n                                borderMode=cv2.BORDER_CONSTANT, borderValue=border)\n        skew_applied = True\n\n    corrected_pil = Image.fromarray(img_np)\n    corrected_images.append(corrected_pil)\n    _record_timing(source_file, 'Orientasi', time.time() - _t_doc)\n\n    # === DISPLAY ===\n    should_display = (MAX_DISPLAY_ORIENT is None) or (i < MAX_DISPLAY_ORIENT)\n\n    if should_display:\n        print(f\"\\n‚úÖ Orientation & Skew Correction gambar {i+1} dari {len(images)}:\")\n        print(f\"   üìÑ File: {source_file}\")\n\n        if osd_error:\n            print(f\"   ‚ö†Ô∏è  OSD Error: {osd_error}\")\n        \n        if rotation_angle == 180 and rotation_applied:\n            print(f\"   üîÑ Rotasi 180¬∞ terdeteksi via confidence check ‚Üí Dikoreksi\")\n        elif rotation_applied:\n            print(f\"   üîç Rotation: {rotation_angle}¬∞ (conf: {rotation_conf:.2f}) ‚Üí Dikoreksi\")\n        else:\n            print(f\"   üîç Rotation: Tidak perlu\")\n\n        print(f\"   üîç Skew: {skew_angle:.2f}¬∞ ‚Üí {'Dikoreksi' if skew_applied else 'Tidak perlu'}\")\n\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n        axes[0].imshow(np.array(images[i]))\n        axes[0].set_title('Original')\n        axes[0].axis('off')\n        axes[1].imshow(np.array(corrected_pil))\n        axes[1].set_title(f'Corrected (rot:{rotation_angle}¬∞ skew:{skew_angle:.1f}¬∞)')\n        axes[1].axis('off')\n        plt.tight_layout()\n        plt.show()\n        print(\"-\" * 60)\n    else:\n        if i == MAX_DISPLAY_ORIENT:\n            print(f\"\\nüìä Memproses gambar {i+1} - {len(images)}...\")\n        print(f\"   ‚úÖ Gambar {i+1} selesai\", end=\"\\r\")\n\nif MAX_DISPLAY_ORIENT is not None and len(images) > MAX_DISPLAY_ORIENT:\n    print(f\"\\n\\nüí° {len(images) - MAX_DISPLAY_ORIENT} gambar lainnya sudah diproses\")\n\nelapsed = time.time() - start_time\nprint(f\"\\n‚è±Ô∏è  Waktu Orientation & Skew Correction: {elapsed:.2f} detik\")\nprint(f\"üìä Total gambar diproses: {len(corrected_images)}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# PREPROCESSING 1: GRAYSCALING\n# ================================================\n\nMAX_DISPLAY_GRAY = 5\n\nprint(\"üîÑ Memulai proses Grayscaling...\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\ngrayscale_images = []\n\nfor i, img in enumerate(corrected_images):\n    _t_doc = time.time()\n    open_cv_image = np.array(img)\n    \n    # Konversi RGB ke Grayscale\n    if len(open_cv_image.shape) == 3:\n        img_gray = cv2.cvtColor(open_cv_image, cv2.COLOR_RGB2GRAY)\n    else:\n        img_gray = open_cv_image\n    \n    grayscale_images.append(img_gray)\n    _record_timing(file_info[i], 'Grayscale', time.time() - _t_doc)\n\n    should_display = (MAX_DISPLAY_GRAY is None) or (i < MAX_DISPLAY_GRAY)\n\n    if should_display:\n        print(f\"\\n‚úÖ Grayscale gambar {i+1} dari {len(corrected_images)}:\")\n        print(f\"   Dimensi: {img_gray.shape[1]} x {img_gray.shape[0]} pixels\")\n        display(Image.fromarray(img_gray))\n        print(\"-\" * 60)\n    else:\n        if i == MAX_DISPLAY_GRAY:\n            print(f\"\\nüìä Memproses gambar {i+1} - {len(corrected_images)}...\")\n        print(f\"   ‚úÖ Gambar {i+1} selesai\", end=\"\\r\")\n\nif MAX_DISPLAY_GRAY is not None and len(corrected_images) > MAX_DISPLAY_GRAY:\n    print(f\"\\n\\nüí° {len(corrected_images) - MAX_DISPLAY_GRAY} gambar lainnya sudah diproses\")\n\nelapsed = time.time() - start_time\nprint(f\"\\n‚è±Ô∏è  Waktu Grayscaling: {elapsed:.2f} detik\")\nprint(f\"üìä Total gambar diproses: {len(grayscale_images)}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# ================================================\n# PREPROCESSING 2: ADAPTIVE NOISE REMOVAL\n# ================================================\nimport matplotlib.pyplot as plt\nMAX_DISPLAY_DENOISE = 5\nprint(\"üîÑ Memulai proses Adaptive Noise Removal...\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\ndenoised_images = []\n\ndef estimate_noise(gray_img):\n    \"\"\"Estimasi noise level menggunakan Laplacian variance.\n    Nilai tinggi = banyak detail/noise, nilai rendah = gambar bersih/halus.\"\"\"\n    return cv2.Laplacian(gray_img, cv2.CV_64F).var()\n\nfor i, gray_img in enumerate(grayscale_images):\n    _t_doc = time.time()\n    \n    # Estimasi noise level untuk pilih metode yang tepat\n    noise_level = estimate_noise(gray_img)\n    \n    if noise_level > 1500:\n        # Gambar sangat noisy (scan kualitas rendah) ‚Üí blur lebih kuat\n        denoised = cv2.GaussianBlur(gray_img, (5, 5), 0)\n        method = \"Gaussian 5x5 (noisy)\"\n    elif noise_level > 500:\n        # Noise sedang ‚Üí blur ringan\n        denoised = cv2.GaussianBlur(gray_img, (3, 3), 0)\n        method = \"Gaussian 3x3 (moderate)\"\n    else:\n        # Gambar bersih (PDF digital) ‚Üí tanpa blur agar teks tetap tajam\n        denoised = gray_img.copy()\n        method = \"No blur (clean)\"\n    \n    denoised_images.append(denoised)\n    _record_timing(file_info[i], 'Denoise', time.time() - _t_doc)\n    \n    should_display = (MAX_DISPLAY_DENOISE is None) or (i < MAX_DISPLAY_DENOISE)\n    if should_display:\n        print(f\"\\n‚úÖ Denoise gambar {i+1} dari {len(grayscale_images)}:\")\n        print(f\"   Dimensi: {denoised.shape[1]} x {denoised.shape[0]} pixels\")\n        print(f\"   üìä Noise level: {noise_level:.0f} ‚Üí {method}\")\n        \n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n        axes[0].imshow(gray_img, cmap='gray')\n        axes[0].set_title('Grayscale')\n        axes[0].axis('off')\n        axes[1].imshow(denoised, cmap='gray')\n        axes[1].set_title(f'Denoised ({method})')\n        axes[1].axis('off')\n        plt.tight_layout()\n        plt.show()\n        print(\"-\" * 60)\n    else:\n        if i == MAX_DISPLAY_DENOISE:\n            print(f\"\\nüìä Memproses gambar {i+1} - {len(grayscale_images)}...\")\n        print(f\"   ‚úÖ Gambar {i+1} selesai\", end=\"\\r\")\n\nif MAX_DISPLAY_DENOISE is not None and len(grayscale_images) > MAX_DISPLAY_DENOISE:\n    print(f\"\\n\\nüí° {len(grayscale_images) - MAX_DISPLAY_DENOISE} gambar lainnya sudah diproses\")\n\nelapsed = time.time() - start_time\nprint(f\"\\n‚è±Ô∏è  Waktu Adaptive Noise Removal: {elapsed:.2f} detik\")\nprint(f\"üìä Total gambar diproses: {len(denoised_images)}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# PREPROCESSING 3: CONTRAST ENHANCEMENT (CLAHE) ‚Äî DINONAKTIFKAN\n# ================================================\n# CATATAN: CLAHE dinonaktifkan karena:\n# 1. Tesseract LSTM (OEM 3) bekerja lebih baik pada grayscale asli\n# 2. CLAHE memperkuat watermark dan artefak latar belakang,\n#    menyebabkan Tesseract membaca watermark sebagai teks\n# 3. Untuk dokumen bersih, CLAHE tidak diperlukan\n#\n# Jika ingin mengaktifkan kembali untuk eksperimen:\n#   clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))\n#   enhanced = clahe.apply(denoised_img)\n\nprint(\"‚è≠Ô∏è  CLAHE dinonaktifkan ‚Äî Tesseract LSTM lebih akurat pada grayscale asli\")\nstart_time = time.time()\n\nenhanced_images = []\nfor i, denoised_img in enumerate(denoised_images):\n    _t_doc = time.time()\n    enhanced_images.append(denoised_img)  # Pass-through tanpa perubahan\n    _record_timing(file_info[i], 'CLAHE', time.time() - _t_doc)\n\nelapsed = time.time() - start_time\nprint(f\"üìä Total gambar: {len(enhanced_images)} (pass-through, {elapsed:.2f} detik)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# PREPROCESSING 4: SHARPENING ‚Äî DINONAKTIFKAN\n# ================================================\n# CATATAN: Sharpening dinonaktifkan karena:\n# 1. Memperkuat noise, watermark, dan artefak JPEG\n# 2. Tesseract LSTM tidak memerlukan penajaman tambahan\n# 3. Pada 300 DPI, teks sudah cukup tajam\n#\n# Jika ingin mengaktifkan kembali untuk eksperimen:\n#   sharpen_kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n#   sharpened = cv2.filter2D(enhanced_img, -1, sharpen_kernel)\n\nprint(\"‚è≠Ô∏è  Sharpening dinonaktifkan ‚Äî dapat memperkuat noise dan watermark\")\nstart_time = time.time()\n\nsharpened_images = []\nfor i, enhanced_img in enumerate(enhanced_images):\n    _t_doc = time.time()\n    sharpened_images.append(enhanced_img)  # Pass-through tanpa perubahan\n    _record_timing(file_info[i], 'Sharpen', time.time() - _t_doc)\n\nelapsed = time.time() - start_time\nprint(f\"üìä Total gambar: {len(sharpened_images)} (pass-through, {elapsed:.2f} detik)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# PREPROCESSING 5: THRESHOLDING ‚Äî DINONAKTIFKAN\n# ================================================\n# CATATAN: Binarisasi (Otsu) dinonaktifkan karena:\n# 1. Tesseract LSTM (OEM 3) dirancang untuk gambar GRAYSCALE, bukan binary\n# 2. Binarisasi menghancurkan informasi gradien yang digunakan LSTM\n#    untuk membedakan karakter mirip (0 vs O, 1 vs l vs I)\n# 3. Referensi: https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html\n#\n# Jika ingin mengaktifkan kembali untuk eksperimen:\n#   _, img_thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\nprint(\"‚è≠Ô∏è  Otsu Thresholding dinonaktifkan ‚Äî LSTM lebih akurat pada grayscale\")\nstart_time = time.time()\n\nthresh_images_adaptive = []\nfor i, img_sharp in enumerate(sharpened_images):\n    _t_doc = time.time()\n    thresh_images_adaptive.append(img_sharp)  # Pass-through tanpa perubahan\n    _record_timing(file_info[i], 'Threshold', time.time() - _t_doc)\n\nelapsed = time.time() - start_time\nprint(f\"üìä Total gambar: {len(thresh_images_adaptive)} (pass-through, {elapsed:.2f} detik)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# PREPROCESSING 6: MORPHOLOGICAL OPERATIONS ‚Äî DINONAKTIFKAN\n# ================================================\n# CATATAN: Operasi morfologi dinonaktifkan karena:\n# 1. Tanpa binarisasi, morfologi tidak dapat diterapkan (butuh gambar binary)\n# 2. Bahkan pada gambar binary, Opening+Closing dengan kernel 2x2\n#    dapat menipis/menebalkan karakter tipis di 300 DPI\n#\n# Jika ingin mengaktifkan kembali (harus aktifkan Otsu juga):\n#   kernel = np.ones((2, 2), np.uint8)\n#   opened = cv2.morphologyEx(thresh_img, cv2.MORPH_OPEN, kernel)\n#   closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n\nprint(\"‚è≠Ô∏è  Morphological Operations dinonaktifkan ‚Äî tidak diperlukan tanpa binarisasi\")\nstart_time = time.time()\n\nmorphed_images = []\nfor i, thresh_img in enumerate(thresh_images_adaptive):\n    _t_doc = time.time()\n    morphed_images.append(thresh_img)  # Pass-through tanpa perubahan\n    _record_timing(file_info[i], 'Morfologi', time.time() - _t_doc)\n\nelapsed = time.time() - start_time\nprint(f\"üìä Total gambar: {len(morphed_images)} (pass-through, {elapsed:.2f} detik)\")\nprint(f\"üí° Final preprocessed images ready for OCR (GRAYSCALE)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# EKSTRAKSI TEKS DENGAN TESSERACT + POST-PROCESSING\n# ================================================\n\nprint(\"üîÑ Memulai ekstraksi teks dengan Tesseract...\")\nprint(\"   Input: Gambar GRAYSCALE (bukan binary)\")\nprint(\"   Config: PSM 3 (auto segmentation) + OEM 3 (LSTM)\")\nprint(\"   Post-processing: Whitespace cleanup + garbage line removal\")\nprint(\"=\" * 60)\nstart_time_total = time.time()\n\nextracted_texts = {}\n\ndef clean_garbage_lines(text):\n    \"\"\"\n    Hapus baris-baris yang kemungkinan besar adalah garbage text\n    (hasil OCR dari watermark, stempel, artefak visual).\n    \n    Kriteria garbage line:\n    1. Baris sangat pendek (<=3 karakter) yang bukan angka/simbol penting\n    2. Baris dengan rasio karakter non-alfanumerik terlalu tinggi\n    3. Baris dengan banyak karakter tunggal terpisah (misalnya \"a b c D e\")\n    \n    Returns:\n        tuple: (cleaned_text, removed_count, removed_examples)\n    \"\"\"\n    lines = text.split('\\n')\n    cleaned_lines = []\n    removed_count = 0\n    removed_examples = []\n    \n    for line in lines:\n        stripped = line.strip()\n        \n        # Baris kosong ‚Üí tetap simpan (untuk mempertahankan paragraf)\n        if not stripped:\n            cleaned_lines.append(line)\n            continue\n        \n        is_garbage = False\n        \n        # --- Cek 1: Baris sangat pendek (1-2 karakter non-bermakna) ---\n        # Misalnya: \"a\", \";\", \"Sa 3\", \"wok ;\"\n        # TAPI jangan buang angka penting seperti \"Rp\", \"VA\", dll\n        if len(stripped) <= 2 and not stripped.isdigit():\n            is_garbage = True\n        \n        # --- Cek 2: Rasio huruf terlalu rendah ---\n        # Baris normal punya banyak huruf (a-z, A-Z) dan angka (0-9)\n        # Garbage text sering punya banyak simbol dan sedikit huruf\n        if not is_garbage and len(stripped) > 0:\n            alnum_count = sum(1 for c in stripped if c.isalnum())\n            alnum_ratio = alnum_count / len(stripped)\n            # Jika kurang dari 40% alfanumerik DAN baris pendek ‚Üí garbage\n            if alnum_ratio < 0.4 and len(stripped) < 20:\n                is_garbage = True\n        \n        # --- Cek 3: Pola karakter tunggal terpisah ---\n        # Misalnya: \"a b c D e F\" atau \"N N P A I a\"\n        # Baris normal punya kata-kata dengan 2+ huruf\n        if not is_garbage:\n            words = stripped.split()\n            if len(words) >= 3:\n                single_char_count = sum(1 for w in words if len(w) == 1)\n                single_char_ratio = single_char_count / len(words)\n                # Jika >60% kata adalah karakter tunggal ‚Üí garbage\n                if single_char_ratio > 0.6 and len(stripped) < 30:\n                    is_garbage = True\n        \n        if is_garbage:\n            removed_count += 1\n            if len(removed_examples) < 10:\n                removed_examples.append(stripped)\n        else:\n            cleaned_lines.append(line)\n    \n    cleaned_text = '\\n'.join(cleaned_lines)\n    return cleaned_text, removed_count, removed_examples\n\nfor i, final_img in enumerate(morphed_images):\n    start_time = time.time()\n    \n    # Tesseract config:\n    # --psm 3 : Fully automatic page segmentation\n    # --oem 3 : LSTM neural network engine (paling akurat)\n    # Input  : Gambar GRAYSCALE\n    text = pytesseract.image_to_string(\n        final_img,\n        lang='ind+eng',\n        config='--psm 3 --oem 3'\n    )\n    \n    # ============================================\n    # POST-PROCESSING\n    # ============================================\n    # 1. Hapus multiple spaces ‚Üí single space\n    text = re.sub(r' +', ' ', text)\n    \n    # 2. Hapus trailing spaces per baris\n    text = re.sub(r' +\\n', '\\n', text)\n    \n    # 3. Hapus garbage lines (watermark, artefak)\n    text, garbage_count, garbage_examples = clean_garbage_lines(text)\n    \n    # 4. Normalize multiple newlines ‚Üí double newline\n    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n    \n    # 5. Trim whitespace awal/akhir\n    text = text.strip()\n    \n    # Simpan hasil\n    source_file = file_info[i]\n    if source_file not in extracted_texts:\n        extracted_texts[source_file] = []\n    extracted_texts[source_file].append(text)\n    \n    elapsed = time.time() - start_time\n    _record_timing(source_file, 'Ekstraksi', elapsed)\n    \n    print(f\"\\n‚úÖ Halaman {i+1}/{len(morphed_images)} (File: {source_file})\")\n    print(f\"   ‚è±Ô∏è  Waktu: {elapsed:.2f} detik | Karakter: {len(text)}\")\n    if garbage_count > 0:\n        print(f\"   üßπ Garbage lines dihapus: {garbage_count}\")\n        if garbage_examples:\n            examples = ', '.join([f'\"{ex}\"' for ex in garbage_examples[:5]])\n            print(f\"      Contoh: {examples}\")\n    print(f\"   üìÑ Preview: {text[:200]}...\")\n    print(\"-\" * 60)\n\nelapsed_total = time.time() - start_time_total\navg_time = elapsed_total / len(morphed_images)\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"‚è±Ô∏è  Total waktu ekstraksi: {elapsed_total:.2f} detik\")\nprint(f\"üìä Rata-rata: {avg_time:.2f} detik/halaman\")\n\nif avg_time < 10:\n    print(f\"‚úÖ Target performa tercapai (<10 detik/dokumen)\")\nelse:\n    print(f\"‚ö†Ô∏è  Performa belum optimal (target: <10 detik)\")\n\nprint(f\"‚úÖ Output disimpan di variable: extracted_texts\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "source": "# ================================================\n# OUTPUT LIST UNTUK MODUL NER\n# ================================================\n# Format output: List of dict, setiap item berisi:\n#   - 'nama_lampiran': Nama file dokumen (string)\n#   - 'hasil_ocr': Teks hasil OCR (string)\n#\n# List ini bisa langsung diteruskan ke modul NER\n# untuk ekstraksi entitas (nominal, IDPEL, nama, dll)\n\nocr_results_list = []\n\nfor filename, ocr_texts in extracted_texts.items():\n    # Gabungkan semua halaman jadi satu teks\n    full_text = '\\n'.join(ocr_texts)\n    \n    ocr_results_list.append({\n        'nama_lampiran': filename,\n        'hasil_ocr': full_text\n    })\n\n# Tampilkan hasil\nprint(\"=\" * 60)\nprint(\"üìã OUTPUT LIST UNTUK MODUL NER\")\nprint(\"=\" * 60)\nprint(f\"\\nüìä Total dokumen: {len(ocr_results_list)}\")\nprint(f\"üì¶ Variable: ocr_results_list\\n\")\n\nfor i, item in enumerate(ocr_results_list, 1):\n    preview = item['hasil_ocr'][:100].replace('\\n', ' ')\n    print(f\"  [{i}] {item['nama_lampiran']}\")\n    print(f\"      Preview: {preview}...\")\n    print(f\"      Panjang: {len(item['hasil_ocr'])} karakter\")\n    print()\n\nprint(\"=\" * 60)\nprint(\"üí° Gunakan ocr_results_list untuk input ke modul NER\")\nprint(\"   Contoh akses:\")\nprint(\"   >>> ocr_results_list[0]['nama_lampiran']\")\nprint(\"   >>> ocr_results_list[0]['hasil_ocr']\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# RINGKASAN WAKTU KESELURUHAN OCR PIPELINE\n# ============================================\nocr_pipeline_total = time.time() - ocr_pipeline_start\n\nprint(f\"{'=' * 120}\")\nprint(f\"‚è±Ô∏è  RINGKASAN WAKTU OCR PIPELINE\")\nprint(f\"{'=' * 120}\")\n\n# Urutan tahap (tanpa CLAHE, Sharpen, Threshold, Morfologi yang dinonaktifkan)\nSTEPS = ['Konversi', 'Orientasi', 'Grayscale', 'Denoise', 'CLAHE', 'Sharpen', 'Threshold', 'Morfologi', 'Ekstraksi']\n\n# Header tabel\nheader = f\"{'No':<4} {'Dokumen':<30} \"\nfor step in STEPS:\n    header += f\"{step:<10} \"\nheader += f\"{'TOTAL':<10}\"\nprint(f\"\\n{header}\")\nprint(\"-\" * 120)\n\n# Isi tabel per dokumen\ndoc_totals = []\nfor idx, (doc_name, steps) in enumerate(timing_per_doc.items(), 1):\n    row = f\"{idx:<4} {doc_name:<30} \"\n    total = 0\n    for step in STEPS:\n        t = steps.get(step, 0)\n        total += t\n        row += f\"{t:<10.2f} \"\n    row += f\"{total:<10.2f}\"\n    doc_totals.append(total)\n    print(row)\n\n# Rata-rata\nprint(\"-\" * 120)\navg_row = f\"{'RATA-RATA':<34} \"\nfor step in STEPS:\n    avg_val = sum(timing_per_doc[d].get(step, 0) for d in timing_per_doc) / len(timing_per_doc)\n    avg_row += f\"{avg_val:<10.2f} \"\navg_total = sum(doc_totals) / len(doc_totals) if doc_totals else 0\navg_row += f\"{avg_total:<10.2f}\"\nprint(avg_row)\nprint(\"=\" * 120)\n\nprint(f\"\\n‚è±Ô∏è  Total waktu keseluruhan OCR pipeline: {ocr_pipeline_total:.2f} detik\")\nprint(f\"üìä Rata-rata per dokumen: {avg_total:.2f} detik\")\n\nif avg_total < 10:\n    print(f\"‚úÖ Target performa tercapai (<10 detik/dokumen)\")\nelse:\n    print(f\"‚ö†Ô∏è  Performa belum optimal (target: <10 detik/dokumen)\")\n\nprint(f\"\\nüí° Catatan: CLAHE, Sharpen, Threshold, Morfologi dinonaktifkan (pass-through ~0.00 detik)\")\nprint(f\"üí° Satuan waktu: detik (seconds)\")\nprint(\"=\" * 120)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengujian Akurasi, WER, dan CER menggunakan jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# FUNGSI UNTUK MENGHITUNG METRIK (SINGLE-LINE)\n# ================================================\n# \n# METRIK EVALUASI OCR:\n# ==========================================\n# 1. Raw Document Similarity (SequenceMatcher)\n#    - Mengukur kesamaan keseluruhan antara ground truth dan OCR output\n#    - Berbasis Longest Common Subsequence (LCS)\n#    - Skala: 0-100% (semakin tinggi semakin baik)\n#    - Kelebihan: Intuitif, menggambarkan \"seberapa mirip\" kedua teks\n#    - Kekurangan: Sensitif terhadap teks tambahan (insertion)\n#\n# 2. WER (Word Error Rate) ‚Äî via jiwer\n#    - Mengukur error pada level KATA\n#    - Menghitung: (Substitusi + Insersi + Delesi) / Total kata di ground truth\n#    - Skala: 0%+ (semakin rendah semakin baik, bisa >100% jika banyak insertion)\n#    - Cocok untuk: Mengevaluasi apakah kata-kata kunci terbaca benar\n#\n# 3. CER (Character Error Rate) ‚Äî via jiwer\n#    - Mengukur error pada level KARAKTER\n#    - Menghitung: (Substitusi + Insersi + Delesi) / Total karakter di ground truth\n#    - Skala: 0%+ (semakin rendah semakin baik)\n#    - PALING RELEVAN untuk verifikasi nominal (Rp, IDPEL, No. Resi, dll)\n#      karena mendeteksi kesalahan sekecil 1 digit\n#\n# REKOMENDASI: Gunakan KETIGA metrik ‚Äî CER sebagai metrik utama untuk\n# verifikasi nominal, WER sebagai pelengkap level kata, dan Raw Similarity\n# sebagai gambaran umum kesamaan keseluruhan dokumen.\n# ==========================================\n\nfrom difflib import SequenceMatcher\n\ndef normalize_to_single_line(text):\n    \"\"\"\n    Gabung semua whitespace (newline, tab, multiple space) menjadi single space.\n    Untuk evaluasi verifikasi teks, yang penting ISI teks, bukan layout/posisi baris.\n    \"\"\"\n    return re.sub(r'\\s+', ' ', text.strip())\n\ndef calculate_raw_document_similarity(ground_truth, ocr_output):\n    \"\"\"\n    Mengukur similarity dokumen dalam mode SINGLE-LINE.\n    Semua whitespace di-normalize sebelum perbandingan,\n    sehingga perbedaan layout/newline tidak mempengaruhi skor.\n    \n    Menggunakan SequenceMatcher yang berbasis Longest Common Subsequence (LCS).\n    \n    Returns similarity dalam bentuk persentase (0-100%)\n    \"\"\"\n    # Normalize ke single-line sebelum membandingkan\n    gt_line = normalize_to_single_line(ground_truth)\n    ocr_line = normalize_to_single_line(ocr_output)\n    \n    matcher = SequenceMatcher(None, gt_line, ocr_line)\n    similarity = matcher.ratio() * 100\n    \n    matching_blocks = matcher.get_matching_blocks()\n    matching_chars = sum(block.size for block in matching_blocks[:-1])\n    \n    return {\n        'raw_similarity': similarity,\n        'matching_chars': matching_chars,\n        'gt_length': len(gt_line),\n        'ocr_length': len(ocr_line)\n    }\n\ndef calculate_wer_cer_jiwer(ground_truth, ocr_output):\n    \"\"\"\n    Menghitung WER dan CER dalam mode SINGLE-LINE menggunakan jiwer.\n    \n    WER (Word Error Rate): Persentase kata yang salah (substitusi + insersi + delesi)\n    CER (Character Error Rate): Persentase karakter yang salah\n    \n    Nilai lebih rendah = lebih baik. 0% = sempurna.\n    \"\"\"\n    # Normalize ke single-line\n    gt_line = normalize_to_single_line(ground_truth)\n    ocr_line = normalize_to_single_line(ocr_output)\n    \n    # Handle edge case: string kosong\n    if not gt_line or not ocr_line:\n        return {'wer': 100.0, 'cer': 100.0}\n    \n    wer_value = wer(gt_line, ocr_line)\n    cer_value = cer(gt_line, ocr_line)\n    \n    return {\n        'wer': wer_value * 100,\n        'cer': cer_value * 100\n    }\n\nprint(\"‚úÖ Fungsi metrik pengujian siap digunakan (mode: SINGLE-LINE)\")\nprint(\"üì¶ Raw Document Similarity: SequenceMatcher (kesamaan keseluruhan)\")\nprint(\"üì¶ WER: jiwer library (error level kata)\")\nprint(\"üì¶ CER: jiwer library (error level karakter ‚Äî paling relevan untuk verifikasi nominal)\")\nprint(\"üìù Semua perbandingan dilakukan dalam mode single-line (layout diabaikan)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# PENGUJIAN AKURASI, WER, DAN CER (SINGLE-LINE)\n# ================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìä PENGUJIAN AKURASI OCR (Mode: Single-Line)\")\nprint(\"=\" * 80)\n\ntesting_results = []\n\nfor filename, ocr_texts in extracted_texts.items():\n    print(f\"\\n{'=' * 80}\")\n    print(f\"üìÑ Testing File: {filename}\")\n    print(f\"{'=' * 80}\")\n    \n    if filename not in GROUND_TRUTH:\n        print(f\"‚ö†Ô∏è  Ground truth tidak tersedia untuk {filename}\")\n        continue\n    \n    ground_truth = GROUND_TRUTH[filename]\n    ocr_output = '\\n'.join(ocr_texts)\n    \n    # Tampilkan info single-line\n    gt_single = normalize_to_single_line(ground_truth)\n    ocr_single = normalize_to_single_line(ocr_output)\n    \n    print(f\"\\nüìè Informasi Dasar (Single-Line):\")\n    print(f\"   Ground Truth: {len(gt_single)} karakter\")\n    print(f\"   OCR Output:   {len(ocr_single)} karakter\")\n    print(f\"   Selisih:      {abs(len(gt_single) - len(ocr_single))} karakter\")\n    \n    # Hitung Raw Document Similarity\n    similarity_metrics = calculate_raw_document_similarity(ground_truth, ocr_output)\n    print(f\"\\nüìä Raw Document Similarity: {similarity_metrics['raw_similarity']:.2f}%\")\n    print(f\"   Matching Characters: {similarity_metrics['matching_chars']}\")\n    \n    # Hitung WER dan CER\n    wer_cer_metrics = calculate_wer_cer_jiwer(ground_truth, ocr_output)\n    print(f\"üìä WER: {wer_cer_metrics['wer']:.2f}%\")\n    print(f\"üìä CER: {wer_cer_metrics['cer']:.2f}%\")\n    \n    # Simpan hasil\n    result = {\n        'filename': filename,\n        'ground_truth_length': len(gt_single),\n        'ocr_output_length': len(ocr_single),\n        'raw_similarity': similarity_metrics['raw_similarity'],\n        'matching_chars': similarity_metrics['matching_chars'],\n        'wer': wer_cer_metrics['wer'],\n        'cer': wer_cer_metrics['cer'],\n        'ground_truth': ground_truth,\n        'ocr_output': ocr_output\n    }\n    testing_results.append(result)\n    \n    # Preview perbandingan single-line (150 karakter pertama)\n    print(f\"\\nüìù Preview Single-Line (150 karakter pertama):\")\n    print(f\"   GT:  {gt_single[:150]}...\")\n    print(f\"   OCR: {ocr_single[:150]}...\")\n\nprint(f\"\\n\\n{'=' * 80}\")\nprint(\"‚úÖ PENGUJIAN SELESAI\")\nprint(f\"{'=' * 80}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# RINGKASAN HASIL PENGUJIAN (SINGLE-LINE)\n# ================================================\n\nif testing_results:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üìä RINGKASAN HASIL PENGUJIAN SEMUA DOKUMEN (Single-Line)\")\n    print(\"=\" * 80)\n    \n    avg_raw_sim = sum(r['raw_similarity'] for r in testing_results) / len(testing_results)\n    avg_wer = sum(r['wer'] for r in testing_results) / len(testing_results)\n    avg_cer = sum(r['cer'] for r in testing_results) / len(testing_results)\n    \n    print(f\"\\n{'No':<4} {'Dokumen':<35} {'Sim (%)':<12} {'WER (%)':<12} {'CER (%)':<12}\")\n    print(\"-\" * 80)\n    \n    for i, result in enumerate(testing_results, 1):\n        print(f\"{i:<4} {result['filename']:<35} \"\n              f\"{result['raw_similarity']:<12.2f} \"\n              f\"{result['wer']:<12.2f} \"\n              f\"{result['cer']:<12.2f}\")\n    \n    print(\"-\" * 80)\n    print(f\"{'RATA-RATA':<39} {avg_raw_sim:<12.2f} {avg_wer:<12.2f} {avg_cer:<12.2f}\")\n    print(\"=\" * 80)\n    \n    print(f\"\\nüìà Interpretasi Hasil:\")\n    \n    if avg_raw_sim >= 95:\n        print(f\"   ‚úÖ Raw Document Similarity: SANGAT BAIK ({avg_raw_sim:.2f}%)\")\n    elif avg_raw_sim >= 90:\n        print(f\"   ‚úÖ Raw Document Similarity: BAIK ({avg_raw_sim:.2f}%)\")\n    elif avg_raw_sim >= 85:\n        print(f\"   ‚ö†Ô∏è  Raw Document Similarity: CUKUP ({avg_raw_sim:.2f}%)\")\n    else:\n        print(f\"   ‚ùå Raw Document Similarity: KURANG ({avg_raw_sim:.2f}%)\")\n    \n    if avg_wer <= 10:\n        print(f\"   ‚úÖ WER: SANGAT BAIK ({avg_wer:.2f}%)\")\n    elif avg_wer <= 20:\n        print(f\"   ‚úÖ WER: BAIK ({avg_wer:.2f}%)\")\n    elif avg_wer <= 30:\n        print(f\"   ‚ö†Ô∏è  WER: CUKUP ({avg_wer:.2f}%)\")\n    else:\n        print(f\"   ‚ùå WER: KURANG ({avg_wer:.2f}%)\")\n    \n    if avg_cer <= 5:\n        print(f\"   ‚úÖ CER: SANGAT BAIK ({avg_cer:.2f}%)\")\n    elif avg_cer <= 10:\n        print(f\"   ‚úÖ CER: BAIK ({avg_cer:.2f}%)\")\n    elif avg_cer <= 15:\n        print(f\"   ‚ö†Ô∏è  CER: CUKUP ({avg_cer:.2f}%)\")\n    else:\n        print(f\"   ‚ùå CER: KURANG ({avg_cer:.2f}%)\")\n    \n    print(f\"\\nüí° Catatan:\")\n    print(f\"   ‚Ä¢ Mode evaluasi: SINGLE-LINE (layout/newline diabaikan)\")\n    print(f\"   ‚Ä¢ Raw Similarity: Kesamaan keseluruhan teks (0-100%)\")\n    print(f\"   ‚Ä¢ WER: Kesalahan level kata (semakin rendah semakin baik)\")\n    print(f\"   ‚Ä¢ CER: Kesalahan level karakter (semakin rendah semakin baik)\")\n\nelse:\n    print(\"\\n‚ö†Ô∏è  Tidak ada hasil pengujian. Pastikan ground truth sudah dikonfigurasi.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# ANALISIS DETAIL PER DOKUMEN\n# ================================================\n# Menampilkan perbandingan teks ground truth vs OCR output\n# untuk membantu identifikasi pola error dan area perbaikan.\n\nif testing_results:\n    print(\"=\" * 80)\n    print(\"üîç ANALISIS DETAIL PER DOKUMEN\")\n    print(\"=\" * 80)\n    \n    for i, result in enumerate(testing_results, 1):\n        print(f\"\\n{'‚îÄ' * 80}\")\n        print(f\"üìÑ [{i}] {result['filename']}\")\n        print(f\"   Similarity: {result['raw_similarity']:.2f}% | \"\n              f\"WER: {result['wer']:.2f}% | CER: {result['cer']:.2f}%\")\n        print(f\"   GT: {result['ground_truth_length']} chars | \"\n              f\"OCR: {result['ocr_output_length']} chars | \"\n              f\"Selisih: {result['ocr_output_length'] - result['ground_truth_length']:+d} chars\")\n        print(f\"{'‚îÄ' * 80}\")\n        \n        gt_single = normalize_to_single_line(result['ground_truth'])\n        ocr_single = normalize_to_single_line(result['ocr_output'])\n        \n        # Tampilkan teks lengkap (single-line, dipotong jika terlalu panjang)\n        max_display = 500\n        print(f\"\\n   üìó Ground Truth (single-line):\")\n        if len(gt_single) > max_display:\n            print(f\"   {gt_single[:max_display]}... [{len(gt_single)} total chars]\")\n        else:\n            print(f\"   {gt_single}\")\n        \n        print(f\"\\n   üìò OCR Output (single-line):\")\n        if len(ocr_single) > max_display:\n            print(f\"   {ocr_single[:max_display]}... [{len(ocr_single)} total chars]\")\n        else:\n            print(f\"   {ocr_single}\")\n        \n        # Hitung kata yang tepat cocok vs tidak\n        gt_words = set(gt_single.lower().split())\n        ocr_words = set(ocr_single.lower().split())\n        common = gt_words & ocr_words\n        missing = gt_words - ocr_words\n        extra = ocr_words - gt_words\n        \n        print(f\"\\n   üìä Analisis Kata:\")\n        if gt_words:\n            print(f\"      Kata cocok:     {len(common)}/{len(gt_words)} \"\n                  f\"({len(common)/len(gt_words)*100:.0f}%)\")\n        if missing:\n            missing_sample = list(missing)[:10]\n            print(f\"      Kata hilang:    {len(missing)} ‚Äî contoh: {', '.join(missing_sample)}\")\n        if extra:\n            extra_sample = list(extra)[:10]\n            print(f\"      Kata tambahan:  {len(extra)} ‚Äî contoh: {', '.join(extra_sample)}\")\n    \n    # Ringkasan keseluruhan\n    print(f\"\\n\\n{'=' * 80}\")\n    print(f\"üìä KESIMPULAN ANALISIS\")\n    print(f\"{'=' * 80}\")\n    \n    best = max(testing_results, key=lambda r: r['raw_similarity'])\n    worst = min(testing_results, key=lambda r: r['raw_similarity'])\n    \n    print(f\"\\n   üèÜ Terbaik:  {best['filename']} ({best['raw_similarity']:.2f}%)\")\n    print(f\"   üìâ Terburuk: {worst['filename']} ({worst['raw_similarity']:.2f}%)\")\n    \n    avg_sim = sum(r['raw_similarity'] for r in testing_results) / len(testing_results)\n    avg_cer_val = sum(r['cer'] for r in testing_results) / len(testing_results)\n    \n    print(f\"\\n   üìà Rata-rata Similarity: {avg_sim:.2f}%\")\n    print(f\"   üìà Rata-rata CER: {avg_cer_val:.2f}%\")\n    \n    if avg_cer_val <= 10:\n        print(f\"\\n   ‚úÖ CER di bawah 10% ‚Äî cukup baik untuk verifikasi nominal\")\n    elif avg_cer_val <= 20:\n        print(f\"\\n   ‚ö†Ô∏è  CER 10-20% ‚Äî perlu perbaikan untuk verifikasi yang akurat\")\n    else:\n        print(f\"\\n   ‚ùå CER di atas 20% ‚Äî masih perlu optimasi lebih lanjut\")\n    \n    print(f\"\\n   üí° Tips peningkatan:\")\n    print(f\"      ‚Ä¢ Kualitas scan/foto sangat mempengaruhi hasil OCR\")\n    print(f\"      ‚Ä¢ Dokumen dengan watermark/stempel cenderung memiliki CER lebih tinggi\")\n    print(f\"      ‚Ä¢ Dokumen digital bersih (seperti PALANGGA) bisa mencapai CER <1%\")\n\nelse:\n    print(\"‚ö†Ô∏è  Tidak ada hasil testing untuk dianalisis.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}